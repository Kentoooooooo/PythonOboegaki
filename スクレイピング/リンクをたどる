ğŸ”µã‚¯ãƒ­ãƒ¼ãƒ©ã‚’ä½¿ã£ã¦ãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚‹
  æ¬¡ã¯ã€å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚å•†å“ãƒšãƒ¼ã‚¸URLã®ãƒªãƒ³ã‚¯ã‚’ãŸã©ã£ã¦ã€å•†å“æƒ…å ±ã‚’å–å¾—/è§£æã—ã¦CSVã«ä¿å­˜ã™ã‚‹
  
ğŸ”µãƒã‚¤ãƒ³ãƒˆ
  ãƒ»Scrapyã§æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹ã¨ãã¯scrapy.Requestã‚’åˆ©ç”¨ã™ã‚‹
  ãƒ»URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã€scrapy.Requestã®callbackå¼•æ•°ã«æ¸¡ã—ãŸãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã€‚
  
  
ğŸ”µå®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰

import scrapy                    â† Scrapyã‚’import


class FollowLinksSpider(scrapy.Spider):
    name = 'follow_links'                      â† å®Ÿè¡Œæ™‚ã®åå‰ã‚’å®šç¾©
    allowed_domains = ['docs.pyq.jp']
    start_urls = [
        'https://docs.pyq.jp/_static/assets/scraping/item-list.html'        â† æœ€åˆã«èª¿ã¹ã‚‹URL
    ]

    def parse(self, response):
        div_item_list = response.css('div.item-list')[0]        â† åˆã‚ã«item-listã‚’å–å¾—
        for div_item in div_item_list.css('div.item'):          â† ãã®ä¸­ã®itemã‚’è¦‹ã¤ã‘ã‚‹
            # å•†å“URLã®å–å¾—
            item_url = div_item.css('a.item-name::attr(href)').extract_first()     â† itemã®ä¸­ã®item-nameã‚’è¦‹ã¤ã‘ã‚‹

            # å•†å“URLã‚’å€‹åˆ¥ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            yield scrapy.Request(item_url, callback=self.parse_item_page)        â† è¦‹ã¤ã‘ãŸitem_urlã«å¯¾ã—ã¦ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®parse_item_pageã‚’æ™‚åˆ»

    def parse_item_page(self, response):     â† æ¬¡ã«ãŸã©ã‚‹ãƒªãƒ³ã‚¯ã«å¯¾ã—ã¦è¡Œã†å‡¦ç†
        # å•†å“ãƒšãƒ¼ã‚¸ã®HTMLè§£æ

        # å•†å“URLã®å–å¾—
        item_url = response.url  â† responseã§å—ã‘å–ã£ãŸURLã‚’è¨­å®šã™ã‚‹

        # å•†å“åã®å–å¾—
        item_name = response.css('h1.item-name::text').extract_first()       â† å¿…è¦ãªæƒ…å ±ã‚’å–å¾—ã™ã‚‹
        item_name = item_name.strip()                                        â† æ”¹è¡Œæ–‡å­—ã‚’æ¶ˆã™

        # ãƒ¡ãƒ¼ã‚«ãƒ¼ã®å–å¾—
        item_maker = response.css('span.item-maker::text').extract_first()
        item_maker = item_maker.strip()

        # ä¾¡æ ¼ã®å–å¾—
        item_price = response.css('span.item-price::text').extract_first()
        item_price = item_price.strip()

        # åœ¨åº«ã®å–å¾—
        item_stock = response.css('span.item-stock::text').extract_first()
        item_stock = item_stock.strip()

        # ç™ºå£²æ—¥ã®å–å¾—
        item_release_date = response.css('span.item-release-date::text').extract_first()
        item_release_date = item_release_date.strip()

        # è§£æã—ãŸå†…å®¹ã‚’è¾æ›¸ã«ã™ã‚‹
        item_info = {                              â† æœ€å¾Œã«è¾æ›¸ã«ã™ã‚‹ã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›¸ãè¾¼ã¾ã‚Œã‚‹
            'name': item_name,
            'maker': item_maker,
            'price': item_price,
            'stock': item_stock,
            'release_date': item_release_date,
            'url': item_url,
        }
        return item_info


ğŸ”µyieldã«ã¤ã„ã¦
  yieldã¯å€‹åˆ¥ã«HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹
  yield.scrapy.Request(item_url, callback=self.parse_item_page)
  
  yieldã¯é–¢æ•°ã‚’ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã«ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã€‚Scrapyã§scrapy.Requestã‚’åˆ©ç”¨ã™ã‚‹ã¨ãã¯ã€
  yieldã‚’ä½¿ã†ã“ã¨ãŒè¦æ±‚ã•ã‚Œã‚‹ã€‚
  scrapy.Requestå•†å“ãƒšãƒ¼ã‚¸URLã¨URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚ã¨ã«å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒ‡å®šã™ã‚‹ã€‚
  å•†å“ãƒšãƒ¼ã‚¸ã®HTMLãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã¨ã€parse_item_patgeãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã€‚
  parse_item_pageãƒ¡ã‚½ãƒƒãƒ‰ã¯å•†å“ãƒšãƒ¼ã‚¸ã®HTMLã‚’responseã¨ã—ã¦å—ã‘å–ã£ã¦å‡¦ç†ã‚’ã—ã¦ã„ã‚‹ã€‚
  å‰ã®èª²é¡Œã¨åŒæ§˜ã«ã€response.cssãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ©ç”¨ã—ã¦HTMLè§£æã‚’ã—ãŸå¾Œã«ã€è¾æ›¸ãƒ‡ãƒ¼ã‚¿(item_info)ã‚’è¿”ã—ã¦ã„ã‚‹ã€‚
  ãã®ãƒ‡ãƒ¼ã‚¿ãŒ-oã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®šã•ã‚ŒãŸCSVã«ä¿å­˜ã•ã‚Œã‚‹ã€‚
  
  
